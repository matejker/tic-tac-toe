<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="main.css">
    <title>Tic-Tac-Toe</title>
</head>
<body>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<div id="main">
    <h1>Tic-Tac-Toe</h1>
    <em class="description">
        or how to achieve an optimal strategy with Reinforcement learning, decision trees... and bunch of matchboxes
    </em>
    <h2>Chapters</h2>
    <ol>
        <li><a href="#what">What?!</a></li>
        <li><a href="#algebra">Board, groups, symmetries and ternary numbers</a></li>
        <li><a href="#">Decision trees</a></li>
        <li><a href="#menace">MENACE or a pile of matchboxes mastering the game</a></li>
        <li><a href="#reinforcement-learning">Reinforcement learning</a></li>
    </ol>
    <h2 id="what">What?!</h2>
    <p>
        <em>Tic-tac-toe</em> or in british english <em>noughts and crosses</em> is an ancient game which
        <em>every</em> seven year old children learns how to play and not to lose in exactly nine plays or so. Why would
        you spend your time with playing and studying such a triviality? Well, I got interested in Reinforcement learning
        <sup><a href="#rl-note">1)</a></sup> recently. So I can used it for
        <a href="https://github.com/matejker/controllability-of-complex-networks">some more nasty stuff</a> and
        <em>tic-tac-toe</em> is a classic example such that even Sutton & Barto [3] don't hesitate to put into the book's
        introduction.
    </p>
    <p>
        As well as many other <em>trendy</em> machine learning techniques were invented in the early 60s [4], so does
        Reinforcement learning. I was <em>shocked</em> (ok, positively surprised) when I watched one of Hanna Fry's
        Christmas lectures where <a href="https://www.youtube.com/watch?v=TtisQ9yZ2zo&t=890s">Matt Parker came
        with a pile of matchboxes</a> which could play <em>noughts and crosses</em>. For my surprise it wasn't Matt
        Parker who invented it, not even <em>in-that-time PhD student</em> Matthew Scroggs [5] who he refers in his
        <a href="">previous video</a>. It was a former Bletchley Park researcher Donald Michie in 1960 who came up with
        a smart way how to teach a bunch of matchboxes with beads in it and to play <em>noughts and crosses</em> [1, 2].
    </p>
    <p>
        In this <em>article</em>, I try to explain some valuable math properties about the game and try to
        come up with a few ways how to teach a computer to play <em>tic-tac-toe</em>.
    </p>

    <h2 id="algebra">Board, groups, symmetries and ternary numbers</h2>
    <p>
        The <em>tic-tac-toe</em> board is a 3 &times; 3 square-shaped grid. For simplicity and future references,
        let's denote all the fields by numbers 0-8, where the center position is 0. In each turn, new "O" <em>noughts</em>
        and "X" <em>crosses</em> are added and the board is filling up. Some of the board settings are <em>indentical</em>
        up-to-some transformation. As you can see on the picture bellow, both of the boards are <em>equal</em> - the
        second one is rotated 90&deg; to the left.
    </p>
    <div class="center"><img src="order.png"></div>
    <p>
        In abstract algebra, we call such operations under which the object is invariant a <strong>symmetry group</strong>.
        In the <em>tic-tac-toe</em> case, the underlying shape of the board is a square, therefore, we would be
        interested in symmetry group of square. Figure bellow shows that we can reflect the square grid about four axes:
        <em>t<sub>x</sub>, t<sub>y</sub>, t<sub>AC</sub></em> and <em>t<sub>BD</sub></em>, and rotate entire grid about
        90&deg; <em>r</em>, 180&deg; <em>r<sup>2</sup></em> and 270&deg; <em>r<sup>3</sup></em>, and still get the
        <em>same</em> square.
    </p>
    <div class="side-note">
        <p>
            This group is called <em>Dihedral Group D<sub>4</sub></em> and it has 8 elements. Generated by
            âŸ¨a,b:a<sup>4</sup>=b<sup>2</sup>=2,ab=baâˆ’1âŸ©
            For more details see <a href="https://proofwiki.org/wiki/Definition:Dihedral_Group_D4">ProofWiki</a>.
        </p>
    </div>
    <div class="center"><img src="symmetries.png"></div>
    <h3>Permutations</h3>
    <p>
        Each of those <em>group elements</em> {<em>e, r, r<sup>2</sup>, r<sup>3</sup>, t<sub>x</sub>, t<sub>y</sub>,
        t<sub>AC</sub>, t<sub>BD</sub></em>} has its own <em>permutation</em> <sup><a href="#permutation-note">2)</a></sup>
        or a mapping for each element projection. For example, an element <em>r</em> rotates the board about 90&deg; to the right,
        therefore, 0 field maps back to 0, 1 goes to 7, 2->8, 3->1, 4->2, 5->3, 6->4, 7->5, 8->6. However, such notation is a bit
        confusing we can adopt widely used one - (0, 7, 8, 1, 2, 3, 4, 5, 6), where position or <em>index</em> is the
        <em>origin</em> and value is the <em>target</em>. In the following table, we can see all the elements'
        permutations.
    </p>
    <div class="side-note">
        <p>
            Worth noticing that <em>tic-tac-toe permutations</em> do not form the entire permutation group for 8-element
            set <em>S<sub>8</sub></em>. Some permutations would <em>deform</em> the square grid, e.g.
            applying (1, 2) on the board changes it such that 1 is no longer neighbour with 8, etc.
            For more details see
            <a href="https://proofwiki.org/wiki/Cycle_Notation/Examples/Permutations_in_S8">ProofWiki</a>.
        </p>
    </div>
    <div class="center">
        <table style="width: 500px; margin: 0 auto;">
            <tr>
                <th>element</th>
                <th>permutation</th>
                <th>short <em>cycle</em> form<sup><a href="#permutation-cycle-note">3)</a></sup></th>
            </tr>
            <tr>
                <td><em>e</em></td>
                <td>(0, 1, 2, 3, 4, 5, 6, 7, 8)</td>
                <td>()</td>
            </tr>
            <tr>
                <td><em>r</em></td>
                <td>(0, 7, 8, 1, 2, 3, 4, 5, 6)</td>
                <td>(7, 8, 1, 2, 3, 4, 5, 6)</td>
            </tr>
            <tr>
                <td><em>r<sup>2</sup></em></td>
                <td>(0, 5, 6, 7, 8, 1, 2, 3, 4)</td>
                <td>(5, 6, 7, 8, 1, 2, 3, 4)</td>
            </tr>
            <tr>
                <td><em>r<sup>3</sup></em></td>
                <td>(0, 3, 4, 5, 6, 7, 8, 1, 2)</td>
                <td>(3, 4, 5, 6, 7, 8, 1, 2)</td>
            </tr>
            <tr>
                <td><em>t<sub>x</sub></em></td>
                <td>(0, 7, 6, 5, 4, 3, 2, 1, 8)</td>
                <td>(1,7) (2, 6) (3, 5)</td>
            </tr>
            <tr>
                <td><em>t<sub>y</sub></em></td>
                <td>(0, 3, 2, 1, 8, 7, 6, 5, 4)</td>
                <td>(1,3) (4, 8) (5, 7)</td>
            </tr>
            <tr>
                <td><em>t<sub>AC</sub></em></td>
                <td>(0, 1, 8, 7, 6, 5, 4, 3, 2)</td>
                <td>(2, 8) (3, 7) (4, 6)</td>
            </tr>
            <tr>
                <td><em>t<sub>BD</sub></em></td>
                <td>(0, 5, 4, 3, 2, 1, 8, 7, 6)</td>
                <td>(1,5) (2, 4) (6, 8)</td>
            </tr>
        </table>
    </div>
    <p>
        As you can see the center field 0 remains fixed for all permutations, which is pretty obvious and gives a clue
        why we chose 0 as the origin and why the rest of numbers go clockwise all around...
    </p>

    <h3>Ternary numbers</h3>
    <div class="side-note">
        <p>
            If we were interested in the game evolution - how "O" and "X" were added in each turn, we could denote the
            game as series of fields, e.g. 032 corresponds to ðŸ‘‡
           <div class="center"> <img src="032.png" style="width: 130px"></div>
        </p>
    </div>

    <p>
        The last remaining piece in this <em>tic-tac-toe-algebra puzzle</em> are ternary numbers. All of us are
        familiar with decimal, hexadecimal and binary numbers, but numbers with base 3 are less common. For the game
        purposes they suits perfectly as they denote board fields with 0 for empty ones, 1 for "O" and 2 for "X". For
        example two boards mentioned before, their ternary numbers are 201000000 and 200010000 respectively.
    </p>

    <p>
        Putting it all together, we saw that some boards are equivalent up to a symmetry, to find all symmetries for
        given board we can use the permutations obtained from the symmetry group of square. This observation
        save us few thousands boards and we can concentrate on a few <em>classes</em> of boards. Lastly, we can use
        handy ternary numbers to denote the boards.
    </p>

    <h2 id="menace">MENACE or a pile of matchboxes mastering the game</h2>
    <p>
        To be honest, when first I discovered MENACE it  was one of <a href="https://giphy.com/embed/wHoE9wJGlfyf8bRxGD">
        those moments</a>. From the current RL AlphaGo-in-the-news perspective, it seems to be a super smart, cool and
        obvious way in the same time of playing the game like that.
    </p>

    <div class="center"><img src="menace.png" style="width: 95%"></div>

    <h2 id="reinforcement-learning">Reinforcement learning</h2>
    <p>
        A fundamental idea of <em>reinforcement learning</em> is to learn from experience with a <em>carrot and
        stick</em> method. Where moves which leads to a positive outcome are awarded with a <em>carrot</em> and
        negative ones are punished with a <em>stick</em>.
    </p>
    <h3>Reward and value function</h3>
    <p>
        There are some moves which obviously lead to the end the game and we can assign its rewards. However, for
        majority of turns we don't know which tend lead to winning the game and which to lose and draw. Because of that
        we will distinguish between <strong>rewards</strong> gives immediate result value and <strong>value
        function</strong> which measures potential future rewards.
    </p>
    <p>
        As we are playing, winning and losing we would like to <em>back propagate</em> rewards of last moves into
        earlier stages of the game. If the agent win the game we will reward it with 1, on the other hand for lost
        and draw we assigns no point. To do so we define a <strong>value function</strong> V(&middot;) which assigns
        potential to every state of board S<sub>t</sub>. In each game we retrospectively update the values using
        this formula.
    </p>
    <div class="center">
        V(S<sub>t</sub>) &leftarrow; V(S<sub>t</sub>) + &alpha; [V(S<sub>t+1</sub>) - V(S<sub>t</sub>)]
    </div>
    <p>
        The value is derived as a difference between value of previous step (or a reward for final steps) and value
        of the current step. It is discounted by a learning rate &alpha;. Initially, we assign 0s or 1s for all
        winning and losing states and 0.5 for all others <em>middle-states</em>.
    </p>

    <h3>Exploration vs exploitation</h3>
    <p>
        If we've played the game purely based on the value function, it is likely that we would stick to play the same
        all over again. Therefore, we will introduce concept of <em>exploration</em> vs <em>exploitation</em>. In
        exploration case we pick an action at random regardless on its value. On the other hand, for exploitation we
        pick the action with highest value. We can alternate those methods at random with &epsilon; probability,
        therefore, we tend to call this method an &epsilon;-greedy <em>policy</em>.
    </p>

    <h2 class="reference">References:</h2>
    <ol class="reference">
        <li>
            Michie, D. (1963), <em>"Experiments on the mechanization of game-learning Part I. Characterization of the model
            and its parameters"</em>, <a href="https://people.csail.mit.edu/brooks/idocs/matchbox.pdf">Link</a>
        </li>
        <li>
            Michie, D. (1961) <em>"Trial and error"</em>. Penguin Science Survey.
        </li>
        <li>
            Sutton, R. S. & Barto, A. G. (2018), <em>"Reinforcement Learning: An Introduction"</em>, The MIT Press.
        </li>
        <li>
            Mitchell, M. (2019), <em>"Artificial intelligence a guide for thinking humans"</em>, Pelican books
        </li>
        <li>
            Scroggs, M. (2015), <em>"MENACE: Machine Educable Noughts And Crosses Engine"</em>,
            <a href="https://www.mscroggs.co.uk/blog/19">Blog post</a> &
            <a href="https://www.mscroggs.co.uk/menace/">Game</a>
        </li>
    </ol>

    <ol class="foot-notes">
        <li id="rl-note">
            Reinforcement learning is just a fancy <em>machine learning</em> term for <em>carrot and stick</em> method
            of learning. More in [3]
        </li>
        <li id="permutation-note">A permutation is an arrangement of its elements in a ordered sequence.</li>
        <li id="permutation-cycle-note">
            In cycle form we denote only permutations which maps themself in a consecutive <em>chain of elements</em>
            and leave all elements which maps on themself (fixed points).
        </li>
    </ol>
</div>
<footer>
    &copy; Matej Kerekrety 2021 <a href="https://github.com/matejker/tic-tac-toe"><img src="./github.png"></a>
</footer>

</body>
</html>